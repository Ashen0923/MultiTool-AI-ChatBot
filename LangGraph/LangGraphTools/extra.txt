
import os
import streamlit as st
from dotenv import load_dotenv
load_dotenv()

from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, START
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage
from typing_extensions import TypedDict
from typing import Annotated
from newsapi import NewsApiClient
from langchain.tools import Tool

def newsapi_search(query: str, top_k: int = 3) -> str:
    """Search news using NewsAPI"""
    newsapi = NewsApiClient(api_key=os.getenv("NEWS_API_KEY"))
    try:
        results = newsapi.get_everything(q=query, language='en', page_size=top_k)
        if not results['articles']:
            return "No news found on this topic."
        return "\n\n".join(
            f"**{article['title']}**\n"
            f"Source: {article['source']['name']}\n"
            f"Published: {article['publishedAt'][:10]}\n"
            f"{article['description']}\n"
            f"[Read more]({article['url']})"
            for article in results['articles']
        )
    except Exception as e:
        return f"NewsAPI error: {str(e)}"

# Create LangChain Tool
news_tool = Tool.from_function(
    func=newsapi_search,
    name="NewsAPI",
    description="Useful for finding current news articles. Input should be a search query about recent news."
)

# --- Tool setup (from your notebook) ---
api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=2, doc_content_chars_max=500)
arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv, description="Query arxiv papers")

api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)
wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki)

tavily = TavilySearchResults()

tools = [arxiv, wiki, tavily,news_tool]

# --- LLM setup (from your notebook) ---
llm = ChatGroq(model="qwen-qwq-32b")
llm_with_tools = llm.bind_tools(tools=tools)

# --- LangGraph state and graph setup ---
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

def tool_calling_llm(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

builder = StateGraph(State)
builder.add_node("tool_calling_llm", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))
builder.add_edge(START, "tool_calling_llm")
builder.add_conditional_edges("tool_calling_llm", tools_condition)
builder.add_edge("tools", "tool_calling_llm")
graph = builder.compile()
# --- End of backend setup ---

# --- Streamlit UI ---
st.set_page_config(page_title="Multi-Tool AI Chatbot", page_icon="ðŸ¤–")
st.title("ðŸ¤– Multi-Tool AI Chatbot")
st.caption("Ask about AI news, research papers, Wikipedia topics, and more!")

# Initialize chat history in session state
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat history
for msg in st.session_state.messages:
    role = "user" if msg["role"] == "human" else "assistant"
    with st.chat_message(role):
        st.markdown(msg["content"])

# User input box
if prompt := st.chat_input("Type your message and press Enter..."):
    # Add user message to history
    st.session_state.messages.append({"role": "human", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Prepare conversation for the graph
    state = {"messages": st.session_state.messages}
    # Run the LangGraph
    response = graph.invoke(state)
    # Extract assistant's message(s)
    new_messages = response["messages"][len(st.session_state.messages):]  # Only new messages

    # Add and display assistant responses
    for msg in new_messages:
        st.session_state.messages.append({"role": "assistant", "content": getattr(msg, "content", str(msg))})
        with st.chat_message("assistant"):
            st.markdown(getattr(msg, "content", str(msg)))